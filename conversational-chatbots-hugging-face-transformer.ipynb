{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementing Blenderbot 2.0 and DialoGPT Chatbots\n\nChatbots have gained a lot of popularity in recent years. As the interest grows in using chatbots for business, researchers also did a great job on advancing conversational AI chatbots.\n\nIn this tutorial, we'll use the Huggingface transformers library to employ the pre-trained Blenderbot and DialoGPT models for conversational response generation, then we will have a brief conversation with both about random subjects and evaluate by ourselves if the responses make sense.\n\nFirstly, I have to inform that I prefered quoting a significant amount of the model explanations from official publications because they were extraordinarily well done, so all merit to the authors of them, I just did the implementation, at the end of the notebook I reference the webpages if you want to get more information.\n\nWith no more words to say, let's get started!","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"ngzXFIHAgt49","outputId":"31eda91d-9b22-4a99-b99d-54e041114931","execution":{"iopub.status.busy":"2023-09-13T16:44:02.541384Z","iopub.execute_input":"2023-09-13T16:44:02.542509Z","iopub.status.idle":"2023-09-13T16:44:13.560007Z","shell.execute_reply.started":"2023-09-13T16:44:02.542376Z","shell.execute_reply":"2023-09-13T16:44:13.558734Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.15.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.10.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.2.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Blenderbot 2.0 model\n\nFacebook AI Research has built and open-sourced BlenderBot 2.0, the first chatbot that can simultaneously build long-term memory it can continually access, search the internet for timely information, and have sophisticated conversations on nearly any topic. It’s a significant update to the original BlenderBot, which we open-sourced in 2020 and which broke ground as the first to combine several conversational skills — like personality, empathy, and knowledge — into a single system.\n\n- When talking to people, BlenderBot 2.0 demonstrated that it’s better at conducting longer, more knowledgeable, and factually consistent conversations over multiple sessions than its predecessor, the existing state-of-the-art chatbot.\n\n- The model takes pertinent information gleaned during conversation and stores it in a long-term memory so it can then leverage this knowledge in ongoing conversations that may continue for days, weeks, or even months. The knowledge is stored separately for each person it speaks with, which ensures that no new information learned in one conversation is used in another.\n\n- During conversation, the model can generate contextual internet search queries, read the results, and incorporate that information when responding to people’s questions and comments. This means the model stays up-to-date in an ever-changing world.\n\n- Today we’re releasing the complete model, code, and evaluation setup, as well as two new conversational data sets — human conversations bolstered by internet searches, and multisession chats with people that reference previous sessions — used to train the model, so other researchers can reproduce this work and advance conversational AI research.\n\nCurrent language-generation models such as GPT-3 and Facebook AI’s first version of BlenderBot can articulately express themselves, at least in the context of ongoing conversations, and generate realistic-looking text. But they suffer from very short “goldfish memory,” and any long-term memory they do have is static — it’s limited to what they’ve been previously taught. They can never gain additional knowledge, which is why GPT-3 and BlenderBot believe that NFL superstar Tom Brady is still on the New England Patriots, and don’t know that he won the 2021 Super Bowl with the Tampa Bay Buccaneers. Similarly, it knows about past popular TV shows and movies, but isn’t aware of new series, like WandaVision.\n\nFinally, we have to consider that there are versions of this model according to the number of parameters such as:\n\n- facebook/blenderbot_small-90M\n- facebook/blenderbot-400M-distill\n- facebook/blenderbot-1B-distill\n- facebook/blenderbot-3B\n\nThe use of these models depends on our compute resources being the RAM the most important, fortunately Kaggle allows loading the biggest one so we will choose it for being the most complex and powerful model. ","metadata":{"id":"-zCpmSm6ktcv"}},{"cell_type":"code","source":"from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\nimport torch","metadata":{"id":"5jXPZp66acZ9","execution":{"iopub.status.busy":"2023-09-13T16:44:13.562783Z","iopub.execute_input":"2023-09-13T16:44:13.563166Z","iopub.status.idle":"2023-09-13T16:44:15.553528Z","shell.execute_reply.started":"2023-09-13T16:44:13.563119Z","shell.execute_reply":"2023-09-13T16:44:15.552515Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We have to load the pre-trained model from huggingface and use it in the tokenizer and model appropriate for Blenderbot. ","metadata":{"id":"WKGviQ01lEeh"}},{"cell_type":"code","source":"model_blender = BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-3B\")\ntokenizer_blender = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-3B\")","metadata":{"id":"LS4wJzavgBLO","outputId":"3f8e7d22-6d06-4c89-b064-35b0456766fa","execution":{"iopub.status.busy":"2023-09-13T16:44:15.554932Z","iopub.execute_input":"2023-09-13T16:44:15.555205Z","iopub.status.idle":"2023-09-13T16:49:58.478290Z","shell.execute_reply.started":"2023-09-13T16:44:15.555171Z","shell.execute_reply":"2023-09-13T16:49:58.476703Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0366b833bd714672871729f7a35dc232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/5.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d119a69a2a494461b4945986ff09cacb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/146k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef29f12bb104f41a595b5fa732316ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/61.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab02446e003647feaeaad8665baf699e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157f7a8a757545edb710ff50b12dfe79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/130 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc2e5b46a9247cab8eba75909fbd85e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/302k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f360ead1823a4c32a553bfd7e899e91e"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The following lines will create a loop in which the input message is tokenized and used as argument of the model in order to generate a response that has to be translated (encoded) to natural language:","metadata":{"id":"gJ263TAplzc2"}},{"cell_type":"code","source":"print(\"Type \\\"q\\\" to quit\")\nwhile True:\n    message = input(\"MESSAGE: \")\n    if message in [\"\", \"q\", \"quit\"]:\n        break\n    inputs = tokenizer_blender([message], return_tensors='pt')\n    reply_ids = model_blender.generate(**inputs)\n    print(f\"Blenderbot 2.0 response:     {tokenizer_blender.batch_decode(reply_ids, skip_special_tokens=True)[0]}\")","metadata":{"id":"wTX0K_pXgBnj","execution":{"iopub.status.busy":"2023-09-13T16:50:48.892036Z","iopub.execute_input":"2023-09-13T16:50:48.892373Z","iopub.status.idle":"2023-09-13T17:02:35.518985Z","shell.execute_reply.started":"2023-09-13T16:50:48.892337Z","shell.execute_reply":"2023-09-13T17:02:35.517634Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Type \"q\" to quit\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  \"\"\n"},{"name":"stdout","text":"Blenderbot 2.0 response:      Do you know how to use quotation marks in a sentence? I do not.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  No\n"},{"name":"stdout","text":"Blenderbot 2.0 response:      Do you think it's a good idea? I'm not sure if I should do it\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  yes\n"},{"name":"stdout","text":"Blenderbot 2.0 response:      Do you know anyone who has done it? I'm not sure if I am\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  yes\n"},{"name":"stdout","text":"Blenderbot 2.0 response:      Do you know anyone who has done it? I'm not sure if I am\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  no\n"},{"name":"stdout","text":"Blenderbot 2.0 response:      Do you know anyone who has done it? I know it's not for everyone.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  no\n"},{"name":"stdout","text":"Blenderbot 2.0 response:      Do you know anyone who has done it? I know it's not for everyone.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  q\n"}]},{"cell_type":"markdown","source":"Well, the model did not follow correctly the historic conversation, we can see it was not able to respond about the current number of deaths by COVID-19 and at the end it totally lost sense.\n\nImportant to notice the model took around 30 seconds to respond to each input I gave, this is a considerably long time if we take into account that a customer wouldn't be willing to wait that much just to have a response to their doubts, thus this must be the next challenge to face.","metadata":{}},{"cell_type":"markdown","source":"# DialoGPT model\n\nDialoGPT is a large-scale tunable neural conversational response generation model trained on 147M conversations extracted from Reddit. The good thing is that you can fine-tune it with your dataset to achieve better performance than training from scratch.\n\nThere are three versions of DialoGPT; small, medium, and large. Fortunately Kaggle didn't have any problem loading the largest version, so let's get started:","metadata":{"id":"LnXqBjJekm4L"}},{"cell_type":"markdown","source":"We have to do exactly the same process for this one. Firstly, import the tokenizer and model which will work by loading into them the pre-trained DialoGPT:","metadata":{"id":"1YWLpwvTmKX0"}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_name = \"microsoft/DialoGPT-large\"\n# model_name = \"microsoft/DialoGPT-medium\"\n# model_name = \"microsoft/DialoGPT-small\"\n\ntokenizer_GPT = AutoTokenizer.from_pretrained(model_name)\nmodel_GPT = AutoModelForCausalLM.from_pretrained(model_name)","metadata":{"id":"ZDCDumihgHME","outputId":"931a17ea-9b2c-4cdb-a70b-ad0cfca5b604","trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e5fccdd097424a90e23c87e835e4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba16efffa0bf456b824cc563a3379cc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b74e30d60c544380b53413dc2c059d09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0aa48c1caf34e0796f6e4d81a618e63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab46d2e2c7d44f1d8e7b7977052c47a9"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now, this process is longer than in the previous model, but the logic is the same, the input message is encoded with the tokenizer created, then concatenate with the historic messages and use it as argument of the model so as to generate a response, finally this is translated back to natural language.\n\nThere are some improvements. However, sampling on an exhaustive list of sequences with low probabilities can lead to random generation (like you see in the last sentence).\n\nTo improve it furthermore, we can:\n\n- Lower the sampling temperature that helps us decrease the likelihood of picking low probability words and increase the likelihood of picking high probability words.\n- Use Top-k sampling instead of picking all probable occurrences. This will help us to discard low probability words from getting picked.\n- Nucleus sampling or Top-p sampling chooses from the smallest possible words whose cumulative probability exceeds the parameter p we set.\n- We set do_sample to True for sampling.\n- Set skip_special_tokens to True to make sure we don't see any annoying special tokens such as <|endoftext|>.\n","metadata":{"id":"PrFRcl4xm0FX"}},{"cell_type":"code","source":"step = 0\n\nprint(\"Type \\\"q\\\" to quit\")\nwhile True:\n    text = input(\"MESSAGE: \")\n    if text in [\"\", \"q\", \"quit\"]:\n        break\n    # encode the input and add end of string token\n    input_ids = tokenizer_GPT.encode(text + tokenizer_GPT.eos_token, return_tensors=\"pt\")\n    # concatenate new user input with chat history (if there is)\n    bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1) if step > 0 else input_ids\n    # generate a bot response\n    chat_history_ids = model_GPT.generate(\n        bot_input_ids,\n        max_length=1000,\n        do_sample=True,\n        top_p=0.95,\n        top_k=0,\n        temperature=0.75,\n        pad_token_id=tokenizer_GPT.eos_token_id\n    )\n    #print the output\n    output = tokenizer_GPT.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n    step = step + 1\n    print(f\"DialoGPT response: {output}\")","metadata":{"id":"cQKGK81yg-Oc","outputId":"7c4f21fa-b2eb-4c38-a4af-608916fd602c","trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Type \"q\" to quit\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  \"\"\n"},{"name":"stdout","text":"DialoGPT response: I feel like you might be out of your element.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  how you know?\n"},{"name":"stdout","text":"DialoGPT response: You looked at my flair...\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  seriously?\n"},{"name":"stdout","text":"DialoGPT response: I am a guy that knows many people on the server.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  how to trust you?\n"},{"name":"stdout","text":"DialoGPT response: I am not the person you are asking.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  great\n"},{"name":"stdout","text":"DialoGPT response: I know you know me\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  yes\n"},{"name":"stdout","text":"DialoGPT response: greatness intensifies\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  not really\n"},{"name":"stdout","text":"DialoGPT response: hehe is soooooo adorable\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  getting bored now\n"},{"name":"stdout","text":"DialoGPT response: He is indeed adorable\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  q\n"}]},{"cell_type":"markdown","source":"The model apparently has chosen a not too nice persona, this is why it gave pesimistic responses and didn't want to say much about it. I think it has the problem of memory at the same level as the previous model. \n\nThe unique advantage of this one is that it took around 7 seconds to respond to the input messages. ","metadata":{}},{"cell_type":"markdown","source":"We have to see these technologies in an optimistic way and at first sight definitely both require fine tuning and training on datasets about specific subjects in order to be able to respond more appropriately to the user, but this obviously depends on the purposes a company would like to give it. ","metadata":{}},{"cell_type":"markdown","source":"Finally, I have to give credits to the publications from which I gathered and quoted a big part of the explanations, as they inspired me to tackle this fascinating field I absolutely encourage you to take a look at their content:\n\nhttps://ai.facebook.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/\n\nhttps://www.thepythoncode.com/article/conversational-ai-chatbot-with-huggingface-transformers-in-python\n\n\nI would like to know any feedback in order to increase the performance of the model or tell me if you found a different one even better!\n\nIf you liked this notebook I would appreciate so much your upvote if you want to see more projects/tutorials like this one. I encourage you to see my projects portfolio, am sure you will love it.\n\nThank you!","metadata":{}}]}